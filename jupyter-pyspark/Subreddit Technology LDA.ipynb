{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\r\n"]}],"source":["!scala -version"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["from google.cloud import bigquery\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["'2.4.5'"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["spark = SparkSession.builder \\\n","  .appName('Reddit LDA Topics')\\\n","  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\\n","  .getOrCreate()\n","\n","spark.version"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"]},{"cell_type":"markdown","metadata":{},"source":["# Retrieve Reddit Data from BigQuery"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["QUERY = \"\"\"\n","SELECT *\n","FROM `fh-bigquery.reddit_posts.2018_*`\n","WHERE score>0 \n","and subreddit in (select subr from `fh-bigquery.reddit.top20`)\n","-- and subreddit = 'technology' \n","\"\"\""]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["spark = SparkSession.builder.appName('Query Results').getOrCreate()\n","bq = bigquery.Client()"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Querying BigQuery\n"]},{"data":{"text/plain":["<google.cloud.bigquery.table.RowIterator at 0x7fa2e14288d0>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["print('Querying BigQuery')\n","table_id = \"cptsrewards-hrd.Jason_temp.test_tmp_table\"\n","\n","job_config = bigquery.QueryJobConfig(\n","    allow_large_results=True, destination=table_id, use_legacy_sql=False\n",")\n","job_config.write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE\n","\n","query_job = bq.query(QUERY, job_config=job_config)\n","query_job.result()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["df = spark.read.format('bigquery') \\\n","    .option('dataset', query_job.destination.dataset_id) \\\n","    .load(query_job.destination.table_id)"]},{"cell_type":"markdown","metadata":{},"source":["# Remove Special Characters"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["def ascii_ignore(x):\n","    return x.encode('ascii', 'ignore').decode('ascii')\n","\n","ascii_udf = udf(ascii_ignore)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["df_titles = df.withColumn(\"title_no_ascii\", ascii_udf('title')) \\\n",".withColumn(\"title_no_spaces\", trim(col(\"title_no_ascii\"))) \\\n",".filter('length(title_no_spaces) > 10') \\\n",".select('title')\\\n",".cache()"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"data":{"text/plain":["6713450"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["df_titles.count()"]},{"cell_type":"markdown","metadata":{},"source":["# Text Prepping"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["from pyspark.ml import Pipeline\n","from pyspark.ml.feature import CountVectorizer, Tokenizer, StopWordsRemover, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["# http://spark.apache.org/docs/latest/ml-features.html#tf-idf\n","# https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3741049972324885/3783546674231782/4413065072037724/latest.html\n","\n","tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n","df_tokenizer = tokenizer.transform(df_titles)\n","\n","remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n","df_remover = remover.transform(df_tokenizer)\n","\n","vectorizer = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\",\n","                             minDF=5, vocabSize=100).fit(df_remover)\n","\n","df_titles_out = vectorizer.transform(df_remover)\n","\n","# hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=100)\n","\n","# idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n","\n","# pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf])\n","\n","# model = pipeline.fit(df_titles)\n","# df_titles_out = model.transform(df_titles)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"data":{"text/html":["<table border='1'>\n","<tr><th>title</th><th>words</th><th>filtered</th><th>features</th></tr>\n","<tr><td>MRW I don&#x27;t know ...</td><td>[mrw, i, don&#x27;t, k...</td><td>[mrw, know, react...</td><td>(100,[21],[1.0])</td></tr>\n","<tr><td>Faith &amp;amp; unfal...</td><td>[faith, &amp;amp;, un...</td><td>[faith, &amp;amp;, un...</td><td>(100,[83],[1.0])</td></tr>\n","<tr><td>Look at this so c...</td><td>[look, at, this, ...</td><td>[look, called, &quot;c...</td><td>(100,[72],[1.0])</td></tr>\n","<tr><td>Anon makes an alp...</td><td>[anon, makes, an,...</td><td>[anon, makes, alp...</td><td>(100,[82],[1.0])</td></tr>\n","<tr><td>/g/ anon values h...</td><td>[/g/, anon, value...</td><td>[/g/, anon, value...</td><td>(100,[],[])</td></tr>\n","<tr><td>MRW I look outsid...</td><td>[mrw, i, look, ou...</td><td>[mrw, look, outsi...</td><td>(100,[49,72],[1.0...</td></tr>\n","<tr><td>MRW I&#x27;m finally d...</td><td>[mrw, i&#x27;m, finall...</td><td>[mrw, finally, do...</td><td>(100,[56],[1.0])</td></tr>\n","<tr><td>The group least l...</td><td>[the, group, leas...</td><td>[group, least, li...</td><td>(100,[15],[1.0])</td></tr>\n","<tr><td>Anon deals with bi</td><td>[anon, deals, wit...</td><td>[anon, deals, bi]</td><td>(100,[],[])</td></tr>\n","<tr><td>MRW my post doesn...</td><td>[mrw, my, post, d...</td><td>[mrw, post, take,...</td><td>(100,[68],[1.0])</td></tr>\n","<tr><td>Waiting for the a...</td><td>[waiting, for, th...</td><td>[waiting, angry, ...</td><td>(100,[],[])</td></tr>\n","<tr><td>Anon shits at Bur...</td><td>[anon, shits, at,...</td><td>[anon, shits, bur...</td><td>(100,[],[])</td></tr>\n","<tr><td>When Iranian vega...</td><td>[when, iranian, v...</td><td>[iranian, vegans,...</td><td>(100,[9,53],[1.0,...</td></tr>\n","<tr><td>When i realize it...</td><td>[when, i, realize...</td><td>[realize, speed, ...</td><td>(100,[],[])</td></tr>\n","<tr><td>4chan isn&#x27;t alway...</td><td>[4chan, isn&#x27;t, al...</td><td>[4chan, always, h...</td><td>(100,[91],[1.0])</td></tr>\n","<tr><td>/v/irgin on art</td><td>[/v/irgin, on, art]</td><td>[/v/irgin, art]</td><td>(100,[],[])</td></tr>\n","<tr><td>Do you think we w...</td><td>[do, you, think, ...</td><td>[think, advanced,...</td><td>(100,[15,86],[1.0...</td></tr>\n","<tr><td>Vice President Pe...</td><td>[vice, president,...</td><td>[vice, president,...</td><td>(100,[],[])</td></tr>\n","<tr><td>bong is an absolu...</td><td>[bong, is, an, ab...</td><td>[bong, absolute, ...</td><td>(100,[],[])</td></tr>\n","<tr><td>Evangelist who wa...</td><td>[evangelist, who,...</td><td>[evangelist, want...</td><td>(100,[65],[1.0])</td></tr>\n","</table>\n","only showing top 20 rows\n"],"text/plain":["+--------------------+--------------------+--------------------+--------------------+\n","|               title|               words|            filtered|            features|\n","+--------------------+--------------------+--------------------+--------------------+\n","|MRW I don't know ...|[mrw, i, don't, k...|[mrw, know, react...|    (100,[21],[1.0])|\n","|Faith &amp; unfal...|[faith, &amp;, un...|[faith, &amp;, un...|    (100,[83],[1.0])|\n","|Look at this so c...|[look, at, this, ...|[look, called, \"c...|    (100,[72],[1.0])|\n","|Anon makes an alp...|[anon, makes, an,...|[anon, makes, alp...|    (100,[82],[1.0])|\n","|/g/ anon values h...|[/g/, anon, value...|[/g/, anon, value...|         (100,[],[])|\n","|MRW I look outsid...|[mrw, i, look, ou...|[mrw, look, outsi...|(100,[49,72],[1.0...|\n","|MRW I'm finally d...|[mrw, i'm, finall...|[mrw, finally, do...|    (100,[56],[1.0])|\n","|The group least l...|[the, group, leas...|[group, least, li...|    (100,[15],[1.0])|\n","|  Anon deals with bi|[anon, deals, wit...|   [anon, deals, bi]|         (100,[],[])|\n","|MRW my post doesn...|[mrw, my, post, d...|[mrw, post, take,...|    (100,[68],[1.0])|\n","|Waiting for the a...|[waiting, for, th...|[waiting, angry, ...|         (100,[],[])|\n","|Anon shits at Bur...|[anon, shits, at,...|[anon, shits, bur...|         (100,[],[])|\n","|When Iranian vega...|[when, iranian, v...|[iranian, vegans,...|(100,[9,53],[1.0,...|\n","|When i realize it...|[when, i, realize...|[realize, speed, ...|         (100,[],[])|\n","|4chan isn't alway...|[4chan, isn't, al...|[4chan, always, h...|    (100,[91],[1.0])|\n","|     /v/irgin on art| [/v/irgin, on, art]|     [/v/irgin, art]|         (100,[],[])|\n","|Do you think we w...|[do, you, think, ...|[think, advanced,...|(100,[15,86],[1.0...|\n","|Vice President Pe...|[vice, president,...|[vice, president,...|         (100,[],[])|\n","|bong is an absolu...|[bong, is, an, ab...|[bong, absolute, ...|         (100,[],[])|\n","|Evangelist who wa...|[evangelist, who,...|[evangelist, want...|    (100,[65],[1.0])|\n","+--------------------+--------------------+--------------------+--------------------+\n","only showing top 20 rows"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df_titles_out"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["from pyspark.ml.clustering import LDA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lda = LDA(featuresCol = 'features', k=10, maxIter =10)\n","\n","model = lda.fit(df_titles_out)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# ll = model.logLikelihood(df_titles_out)\n","# lp = model.logPerplexity(df_titles_out)\n","# print(\"The lower bound on the log likelihood of the entire corpus: \" + str(ll))\n","# print(\"The upper bound on perplexity: \" + str(lp))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["The topics described by their top-weighted terms:\n","+-----+--------------------+----------------------------------------------------------------------------------------------------------+\n","|topic|termIndices         |termWeights                                                                                               |\n","+-----+--------------------+----------------------------------------------------------------------------------------------------------+\n","|0    |[2, 7, 20, 23, 25]  |[0.19126216914263053, 0.1283388851512674, 0.07794244276993893, 0.06326187778884601, 0.06222672209580321]  |\n","|1    |[6, 4, 13, 38, 45]  |[0.18737980224453477, 0.17002687019598225, 0.1301715023513198, 0.07206668730650158, 0.06843630829071246]  |\n","|2    |[9, 19, 28, 31, 62] |[0.20974030918213601, 0.14496233517595006, 0.12366635019573312, 0.08430531602993338, 0.06996069179356383] |\n","|3    |[37, 41, 58, 48, 64]|[0.17263959105818222, 0.15921470615794156, 0.13890383010809923, 0.1384581753459524, 0.12087454567734648]  |\n","|4    |[18, 12, 29, 52, 47]|[0.2257671549677399, 0.1798217575964291, 0.16352396385349974, 0.13192077930553026, 0.11882200836499163]   |\n","|5    |[1, 22, 49, 61, 76] |[0.4196247337061629, 0.1940065501189379, 0.11317979055503033, 0.10913566533945418, 0.08673757875371291]   |\n","|6    |[0, 5, 21, 30, 32]  |[0.2323681047534175, 0.16472733827241923, 0.08193672670609943, 0.07572153037068431, 0.07236770550060881]  |\n","|7    |[10, 27, 16, 42, 39]|[0.13582106588004916, 0.07727155573713303, 0.07173692854317427, 0.058442536220404386, 0.05647644190443191]|\n","|8    |[3, 14, 26, 34, 33] |[0.20587663388511018, 0.11571416195533696, 0.08592828215693447, 0.07940945973649581, 0.07569104303591376] |\n","|9    |[11, 8, 15, 24, 51] |[0.11196857298544838, 0.1076050704194695, 0.10488870442425852, 0.07129839562297036, 0.05391954376400948]  |\n","+-----+--------------------+----------------------------------------------------------------------------------------------------------+\n","\n"]}],"source":["# Describe topics.\n","# https://www.zstat.pl/2018/02/07/scala-spark-get-topics-words-from-lda-model/\n","topics = model.describeTopics(5)\n","print(\"The topics described by their top-weighted terms:\")\n","topics.show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["topicIndices = model.describeTopics(maxTermsPerTopic = 5)\n","vocabList = vectorizer.vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["[6, 4, 13, 38, 45]"]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["topics.select(\"termIndices\").collect()[1][0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["0\n","[2, 7, 20, 23, 25]\n","['like', 'new', 'make', 'life', 'got']\n","1\n","[6, 4, 13, 38, 45]\n","['ever', 'reddit,', 'time', 'want', 'anyone']\n","2\n","[9, 19, 28, 31, 62]\n","['get', '[serious]', '|', 'it?', 'help']\n"]}],"source":["# Helper function\n","n_top_words = 3\n","def print_topics(topics, vectorizer, num_topics=10):\n","    words = vectorizer.vocabulary\n","    for i in range(n_top_words):\n","        indices = topics.select(\"termIndices\").collect()[i][0]\n","        print(i)\n","        print(indices)\n","        wordsList = []\n","        for j in indices:\n","            wordsList.append(words[j])\n","        print(wordsList)\n","\n","print_topics(topics, vectorizer, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"data":{"text/plain":["pyspark.sql.dataframe.DataFrame"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["type(topicIndices)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'my_df' is not defined","output_type":"error","traceback":["\u001B[0;31m---------------------------------------------------------------------------\u001B[0m","\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)","\u001B[0;32m<ipython-input-23-41d3fddb7ff8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint_topics\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmy_df\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mvectorizer\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m","\u001B[0;31mNameError\u001B[0m: name 'my_df' is not defined"]}],"source":["print_topics(my_df,vectorizer, 10)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true},"outputs":[],"source":["topics = topicIndices.map(lambda (terms, termWeights): terms.map(vocabList(_)).zip(termWeights))\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["topicIndices"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Shows the result\n","transformed = model.transform(df_titles_out)\n","transformed.show(truncate=False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["transformed.select(\"topicDistribution\").show(10,False)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}