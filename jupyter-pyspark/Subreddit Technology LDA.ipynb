{"cells":[{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Scala code runner version 2.12.10 -- Copyright 2002-2019, LAMP/EPFL and Lightbend, Inc.\r\n"]}],"source":["!scala -version"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["from google.cloud import bigquery\n","from pyspark.sql import SparkSession\n","from pyspark.sql.functions import *"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/plain":["'2.4.5'"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["spark = SparkSession.builder \\\n","  .appName('1.3. BigQuery Storage &  Spark MLlib - Python')\\\n","  .config('spark.jars', 'gs://spark-lib/bigquery/spark-bigquery-latest_2.12.jar') \\\n","  .getOrCreate()\n","\n","spark.version"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["spark.conf.set(\"spark.sql.repl.eagerEval.enabled\",True)"]},{"cell_type":"markdown","metadata":{},"source":["# Retrieve Reddit Data from BigQuery"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[],"source":["QUERY = \"\"\"\n","SELECT *\n","FROM `fh-bigquery.reddit_posts.2018_*`\n","WHERE subreddit = 'technology' AND score >10\n","\"\"\""]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["spark = SparkSession.builder.appName('Query Results').getOrCreate()\n","bq = bigquery.Client()"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Querying BigQuery\n"]},{"data":{"text/plain":["<google.cloud.bigquery.table.RowIterator at 0x7fc78519d2d0>"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["print('Querying BigQuery')\n","query_job = bq.query(QUERY)\n","query_job.result()"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[],"source":["df = spark.read.format('bigquery') \\\n","    .option('dataset', query_job.destination.dataset_id) \\\n","    .load(query_job.destination.table_id)"]},{"cell_type":"markdown","metadata":{},"source":["# Remove Special Characters"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["def ascii_ignore(x):\n","    return x.encode('ascii', 'ignore').decode('ascii')\n","\n","ascii_udf = udf(ascii_ignore)"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["df_titles = df.withColumn(\"title_no_ascii\", ascii_udf('title')) \\\n",".withColumn(\"title_no_spaces\", trim(col(\"title_no_ascii\"))) \\\n",".filter('length(title_no_spaces) > 10') \\\n",".select('title')\\\n",".cache()"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"data":{"text/plain":["126742"]},"execution_count":21,"metadata":{},"output_type":"execute_result"}],"source":["df_titles.count()"]},{"cell_type":"markdown","metadata":{},"source":["# Text Prepping"]},{"cell_type":"code","execution_count":35,"metadata":{},"outputs":[],"source":["from pyspark.ml import Pipeline\n","from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover, StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler"]},{"cell_type":"code","execution_count":39,"metadata":{},"outputs":[],"source":["tokenizer = Tokenizer(inputCol=\"title\", outputCol=\"words\")\n","\n","remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")\n","\n","pipeline = Pipeline(stages=[tokenizer, remover])\n","\n","model = pipeline.fit(df_1)\n","df_2 = model.transform(df_1)"]},{"cell_type":"code","execution_count":41,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|title                                                                                                                                                                                                                                                                              |words                                                                                                                                                                                                                                                                                                                          |filtered                                                                                                                                                                                                                                                     |\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","|The head of the US Federal Communications Commission defended Monday his move to roll back rules requiring internet providers to treat all traffic equally, saying it was needed to encourage investment                                                                           |[the, head, of, the, us, federal, communications, commission, defended, monday, his, move, to, roll, back, rules, requiring, internet, providers, to, treat, all, traffic, equally,, saying, it, was, needed, to, encourage, investment]                                                                                       |[head, us, federal, communications, commission, defended, monday, move, roll, back, rules, requiring, internet, providers, treat, traffic, equally,, saying, needed, encourage, investment]                                                                  |\n","|Law Enforcement Cracking Open iOS Devices Is ‘Threatening The Core Of An iPhone’s Value,’ Snowden Argues: \"The only compelling reason for someone to buy an iPhone over more open, less expensive competitors was Apple's stronger stance on users' right to privacy and security.\"|[law, enforcement, cracking, open, ios, devices, is, ‘threatening, the, core, of, an, iphone’s, value,’, snowden, argues:, \"the, only, compelling, reason, for, someone, to, buy, an, iphone, over, more, open,, less, expensive, competitors, was, apple's, stronger, stance, on, users', right, to, privacy, and, security.\"]|[law, enforcement, cracking, open, ios, devices, ‘threatening, core, iphone’s, value,’, snowden, argues:, \"the, compelling, reason, someone, buy, iphone, open,, less, expensive, competitors, apple's, stronger, stance, users', right, privacy, security.\"]|\n","+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n","only showing top 2 rows\n","\n"]}],"source":["df_2.show(2,False)"]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["from pyspark.ml.clustering import LDA"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lda = LDA(k=10, maxIter =10)\n","\n","model = lda.fit(dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["categorical_columns= ['sex', 'education_level', 'marital_status', 'pay_0','pay_2', 'pay_3', 'pay_4', 'pay_5', 'pay_6']\n","\n","numeric_col = ['age', 'bill_amt_1','bill_amt_2','bill_amt_3','bill_amt_4','bill_amt_5','bill_amt_6',\n","              'pay_amt_1','pay_amt_2','pay_amt_3','pay_amt_4','pay_amt_5','pay_amt_6']\n","\n","\n","# The index of string vlaues multiple columns\n","indexers = [\n","    StringIndexer(inputCol=c, outputCol=\"{0}_indexed\".format(c))\n","    for c in categorical_columns\n","]\n","\n","# The encode of indexed vlaues multiple columns\n","encoders = [OneHotEncoder(dropLast=False,inputCol=indexer.getOutputCol(),\n","            outputCol=\"{0}_encoded\".format(indexer.getOutputCol())) \n","    for indexer in indexers\n","]\n","\n","\n","# Scaling numerical variables\n","scaled_numeric_col = [col + \"_scaled\" for col in numeric_col]\n","numeric_assemblers = [VectorAssembler(inputCols=[col], outputCol=col + \"_vec\") for col in numeric_col]\n","scalers = [StandardScaler(inputCol=col + \"_vec\", outputCol=col + \"_scaled\") for col in numeric_col]\n","\n","# Vectorizing encoded values\n","assembler = VectorAssembler(inputCols=scaled_numeric_col + [encoder.getOutputCol() for encoder in encoders],outputCol=\"features\")\n","\n","pipeline = Pipeline(stages=\n","                        indexers+ \n","                        encoders+\n","                        numeric_assemblers+\n","                        scalers+\n","                        [assembler])\n","model=pipeline.fit(df_cc_data2)\n","df_assembler_output = model.transform(df_cc_data2)\n","\n","df_assembler_output"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"nbformat":4,"nbformat_minor":2}